# Fork Description
This is our research work for WACV 2021.

To get results of the *original baseline*, do not use this repository. Instead use the original [sergeywong/cp-vton](https://github.com/sergeywong/cp-vton).


## WACV Video Virtual Try-On Network

This code is tested with pytorch=1.5.1. Install the environment with `conda env create -f environment.yml`.


## Data preprocessing
<details>
<summary>Manually generate dataset</summary>
<br>
We convert the original data [VITON](https://github.com/xthan/VITON) into different directories for easily use. 

Run the matlab code ```convert_data.m ``` under the original data root ```VITON/data```, and get the new format.

We use the json format for pose info as generated by [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).

Move these directories into our own dataroot ```data```.
</details>



You can get the processed data at [GoogleDrive](https://drive.google.com/open?id=1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo) or by running:

## Warp Module

### Training
We use the same warp network (GMM) as CP-VTON, except we add densepose.

Example:
```bash
echo "Train Warp Module"; \
python train.py \
--name $(date +"%Y-%m-%d_%H-%M-%S")_train_warp \
--model warp \
--person_inputs agnostic densepose \
--vvt_dataroot /data_hdd/fw_gan_vvt \
--workers 32 \
--gpu_ids 0,1,2,3,4,5,6,7 \
--batch_size 128
```

<details>
<summary>View in Tensorboard</summary>
<!--Blank line enable code-->

You can see the results in tensorboard, as show below. 
```bash
tensorboard --logdir experiments  # recommended to do this in a tmux window
```
We can port forward the training like this
```bash
echo "tensorboard connection"; ssh -N -L localhost:6006:localhost:6006 username@10.52.0.34
```

</details>

<div align="center">
  <img src="result/gmm_train_example.png" width="576px" />
    <p>Example of warp train. The center image is the warped cloth.</p>
</div>

### Eval
Choose different data source (train or test) for eval with the option `--datamode`.

An example command is
```bash
python test.py \
--name warp_cloth_for_train_data \
--model warp \
--datamode train \
--dataset vvt \
--vvt_dataroot /data_hdd/vvt_competition \
--checkpoint path/to/checkpoint
```

You can see the results in tensorboard, as show below.

<div align="center">
  <img src="result/gmm_test_example.png" width="576px" />
    <p>Example of warp on the train data. The center image is the warped cloth.</p>
</div>

To prepare to train SAMS below, generate warp-cloth using the test process of 
`--model warp` with `--datamode train` described above. 
**Then move these files or make symlinks under the directory `warp-cloth`.**
```bash
ln -s $(readlink -f test_results/"$CHECKPOINT"/VVTDataset/warp-cloth) warp-cloth
```
## Self-Attentive Multi-Spade
### Training

An example training command is

```bash
echo "Train SAMS Model"; \
python train.py \
--name $(date +"%Y-%m-%d_%H-%M-%S")_train_sams \
--model sams \
--vvt_dataroot /data_hdd/fw_gan_vvt \
--workers 32 \
--gpu_ids 0,1,2,3,4,5,6,7 \
--batch_size 128 \
--person_inputs agnostic densepose \
--self_attn
```
You can see the results in tensorboard, as show below.

<div align="center">
  <img src="result/tom_train_example.png" width="576px" />
    <p>Example of TOM train. The center image in the last row is the synthesized image.</p>
</div>

### Eval
An example training command is
```
python test.py --name sams_test --model sams --workers 4 --datamode test --checkpoint path/to/checkpoint.pth
```


## Citation
If this code helps your research, please cite our paper:

COMING SOON.


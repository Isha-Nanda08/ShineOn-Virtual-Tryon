# III. Train

Our approach contains two models: the core **SAMS-GAN** and the auxiliary WarpModule. We also compare against a baseline UNet-Mask model (based on the TOM model from CP-VTON).


The WarpModule is used to pre-warp the garment image to the shape of the user. It can be treated as a block box. You can just download the pre-warped cloths here (COMING SOON) and place them in `${PROJECT_ROOT}/warp-cloth`. To warp cloths on your own data, we provide pretrained weights for the WarpModule that you can find here (COMING SOON).




<details>
<summary><b>View in Tensorboard</b></summary>

All training progress can be viewed in Tensorboard.
```bash
tensorboard --logdir experiments/
```
We can port forward a Tensorboard connection from a remote server like this:
```bash
ssh -N -L localhost:6006:localhost:6006 username@IP.ADDRESS
```

<br />
</details>


<details>
<summary><b>Common Train Options</b></summary>

Experiment Setup
- `--name` experiment name. Saves checkpoints and logs to `experiments/{name}`
- `--gpu_ids`
- `--workers`

Data
- `--vvt_dataroot` path to FW-GAN VVT Dataset
- `--warp_cloth_dir` path to pre-warped cloths generated by the WarpModule (default: warp-cloth)
- `--batch_size`

Checkpointing and logging
- `--display_count` how often in steps to log to Tensorboard
- `--save_count` how often in steps to save a checkpoint
- `--checkpoint` resume training from this checkpoint (path to `.ckpt` file)

</details>


## SAMS-GAN
<details>
<summary>Instructions</summary>
<br />

A general train command:
```bash
python train.py \
--name "SAMS-GAN_train" \
--model sams \
--ngf_power_start 6 \
--ngf_power_end 10 \
--n_frames_total 5 \
--n_frames_now 1 \
--batch_size 4 \
--num_workers 8
```

### Modules
#### Generator
The SAMS-GAN generator is an encoder-decoder architecture. The outer layers start with higher resolution (hxw) and fewer features. The inner layers have lower resolution and more features. Unlike other models, SAMS does NOT use `--ngf` for generator features.

The number of features in the **outer** layers equals `pow(ngf_power_base,`**`ngf_power_start`**`)`; by default, the outer layers have `2^6=64` features. 

The number of features in the **inner** layers equals `pow(ngf_power_base, `**`ngf_power_end`**`)`; by default, the inner layers have `2^10=1024` features.

Self-attention layers are enabled by default (self-attention is literally part of the acronym SAMS). They can be turned off with `--no_self_attn` to use vanilla Multi-Spade layers. 

#### Discriminators
SAMS-GAN has two discriminators: `Multiscale` that operates on the current frame at different image resolutions, and `Temporal` that operates at the past `--n_frames_now` at a single image resolution.

Discriminator size is uniformly adjusted with `--ndf` (default 64).

### Progressive Training
We use progressive video frame training to speed up generation convergence. We start by generating a single image, then manually increase the number of frames to the max that fits on the GPU.

- `--n_frames_total` . Sets the size of the generation buffer, and how many previous frames are fed into the generator as input. Aim for the max that fits on GPU, 5 or more is ideal. Note that this effectively scales up the batch size; choosing between batch size and n_frames_total is a trade-off.
- `--n_frames_now` . The number of frames to actually train on right now. The rest of the frames are masked with 0s. You should progressively increase this value from 1 up to `--n_frames_total`. 


</details>

## Baseline U-Net Mask (aka TOM)
<details>
<summary>Instructions</summary>
<br />


COMING SOON

</details>

## WarpModule (Optional)
<details>
<summary>Instructions</summary>
<br />


COMING SOON

</details>
